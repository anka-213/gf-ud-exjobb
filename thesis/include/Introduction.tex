% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}

\newcommand{\note}[1]{\todo{#1}}
%\renewcommand{\note}[1]{}

% TODO: Link to the code somewhere

Since the advent of computers, people have been trying to make them understand natural languages. Today machine learning methods are very popular, but the more traditional method is so called rule-based
\ac{NLP}
% Natural Language Processing (NLP)
and Natural Language Generation (NLG), in which we treat natural languages more like programming languages and write the grammar rules explicitly into the computer in order to parse the text into structured data and/or generate text from abstract data.\cite{chomsky-1957,lambek-1958,curry1961some}
In fact, much of the terminology around programming languages comes from linguistics for this exact reason\footnote{Programming \emph{languages} have a \emph{syntax} and a \emph{grammar} which is \emph{parsed} and lexed into \emph{lexemes}}. This work will focus on connecting the two rule-based \ac{NLP} formalisms of \emph{Grammatical Framework}\cite{ranta-2011} and \emph{Universal Dependencies}\cite{nivre-etal-2016-universal}.\footnote{The code is found on github: https://github.com/GrammaticalFramework/gf-ud}


While machine learning is currently dominating in \ac{NLP}, using rule-based approaches still provides several important advantages over those based on machine-learning.
For example, rule-based approaches are deterministic and more predictable. They make it possible to fix individual bugs without needing to retrain the whole model, hoping that it fixes the issue. They are also more transparent, rather than being a black box as most machine learning based language models are. % \todo{When are these properties important? E.g. when correctness is important, (law, medicine)}
These properties are particularly important when correctness is crucial, for example in law and medicine.
% There have been many different approaches to solving...
% Rule-based approaches are deterministic and more predictable. They are also more modular: it is possible to fix a single bug without needing to retrain the whole model. This makes them transparent, instead of being a black box as most machine learning based language models are.

% (why not ai? more predictable, easier to fix bugs, more deterministic)

While technically even the most naive string replacement method\footnote{e.g. adding an ``s'' to the end of a word to make it plural, producing ``foots'' instead of the correct ``feet''} is an instance of ``rule-based \ac{NLP}'', in practice we tend to use more sophisticated tools, namely \emph{grammar formalisms}.

% % Draft
% One problem in \ac{NLP} is to convert from text into mathematical logic, to allow a computer program to deduce things.
%
% % SMU in background or here?
%
% Something converting directly from text to logic is difficult
% Converting from AST to logic is easy (this is what programming languages do)
%
% % Forward references
%
% % Way too specific to come this early!
% Among the most simple and naive methods for handling natural language text is string replacement, e.g. adding an ``s'' to the end of a word to make it plural, regardless of what word it actually is, so we would get ``foots'' instead of the correct ``feet'' as the plural form for ``foot''.
%
% Now, we could keep adding more exceptions to these string replacement rules and get somewhat better results, but we would quickly run into complexity issues with more advanced grammar concepts, like word order changing when asking a question (``The cat is hungry'' vs ``Is the cat hungry?'') and person/number/gender agreement (e.g. person/number: ``I eat''/``The cat eats''/``They eat'' and gender agreement (in Swedish): ``Huset är fint'' vs ``Katten är fin''), so a more systematic approach would be helpful. This is where grammar formalisms come in.
% % End irrelevant stuff

A grammar formalism is used to describe the connection between a plain text and structured data. To understand what a formalism is, it is useful to introduce the concept of structured data. Consider a text string like ``the cat sat on the mat''—for a general-purpose computer program, it is nothing more than an array of characters. But to a human linguist, it is full of structure: ``sat'' is the main verb, ``cat'' is the subject, ``on the mat'' is an adverbial. A grammar formalism is simply a way to represent this grammatical structure in a machine readable way.
%If we want to represent its grammatical structure in machine readable way, we need a \emph{grammar formalism}.


% One method for doing this is to convert the text into an abstract structure that does not care about morphological details like

% Skillnaden mellan strängar och att ha en struktur

% Now that I have introduced grammar formalisms, I need to actually explain them

% TODO: Mention multilinguality of GF


% In rule-based \ac{NLP} we want to be able to take a text and analyse it at a higher level and make transformations to it.
% In order to do this, we first need a way to parse the text into a syntax tree, which describes the relationship between words in a sentence.

% For example you can write a grammar to transform the text into an abstract tree form, which allows understanding its structure and processing it\note{bad}.

% There are many different approaches that have been used to make this process easier. In this thesis we will focus on two such methods, both of which are meant to analyze the grammatical structure and syntax of a sentence.

There are several different formalisms for describing sentences as trees, all with their own strengths and weaknesses.
The two we will be talking about in this paper are Universal Dependencies (UD), which is based on so called \emph{dependency trees}, and Grammatical Framework (GF), which uses \emph{constituency-based abstract syntax trees}.

% \todo[inline]{This paper is about combining these two methods. Move 1.4 here}

% \todo[inline]{Add a figure that shows the difference between Dependency Grammars, Constituency Grammars (a.k.a. phrase structure grammars) and Abstract Syntax Trees}


% \begin{verbatim}
% - Kort om paradigmer
% - Kort om verktygen
% - Vad som är gjort
% - Vad som behöver göras

% Applications:
% - Let a computer understand human language
% - Allow the computer to do things with human writing
% - For example,
% -  - try to understand the meaning
% -  - make transformations on the writing (which?)
% -  - generate human language based on abstract knowledge
% \end{verbatim}

% Reliability and predictability are very important in many applications, for example law.

% \todo[inline]{Write more about the goal of the project and move the rest to background}

% \todo[inline]{Something general about rule-based NLG and \ac{NLP} and computer grammars}

% TODO: Ta reda på vad UD används till

%  - dep trees, UD
\section{Dependency trees and Universal Dependencies}
% \todo[inline]{This is Theory, not intro}
A dependency tree is a tree structure that shows the grammatical relationship between words in a sentence\cite{tesniere2015elements,nivre2006inductive}. Each word becomes a node in the tree with the main verb as the root and the edges represent the relation and the direction of the dependency.

The specific standard for dependency trees that this paper is about is called \ac{UD}\cite{nivre-etal-2016-universal}. UD is based on the idea of making a multilingual standard for dependency trees where the same set of tags can be used regardless of which language the sentence is written in.

In \autoref{fig:cat_sleeps_ud} we can see the sentence ``the cat sleeps'' analyzed as a UD-tree. The first step is to determine which part-of-speech each word in the sentence belongs to. In this case, ``the'' is a determiner, ``cat'' is a noun and ``sleeps'' is a verb. Next we need to determine the relation between the words: ``sleeps'' is the root of the sentence, ``cat'' is the subject of this verb and ``the'' quantifies (determines) the noun.

% Constituency vs dependency trees

% For example in the sentence "John ate an apple", the word "ate" would become the root and "John" and "apple" would be direct children, where the arrow from "ate" to apple would be labeled as the subject relation while the arrow to "apple" would be labeled as the object. There would additionally be an arrow from "apple" to "an" which marks that "an" is a determiner for "apple".

% the root is black. cop stands copula, which is the word "is". nsubj stands for nominal subject and marks the subject of the sentence, that which is black. det stands for determiner, which answers which cat it is that is black

% \todo[inline]{explain this picture}

% Generated by
% echo "the cat sleeps" | gf-ud string2gf2ud grammars/MiniLang Eng S lud
\begin{figure}[H]
\centering
%% the cat is black
% \setlength{\unitlength}{0.2mm}
% \begin{picture}(205.0,90.0)
%   \put(0.0,0.0){the}
%   \put(37.0,0.0){cat}
%   \put(83.0,0.0){is}
%   \put(120.0,0.0){black}
%   \put(0.0,15.0){{\tiny DET}}
%   \put(37.0,15.0){{\tiny NOUN}}
%   \put(83.0,15.0){{\tiny AUX}}
%   \put(120.0,15.0){{\tiny ADJ}}
%   \put(28.5,30.0){\oval(28.89189189189189,33.333333333333336)[t]}
%   \put(14.054054054054054,35.0){\vector(0,-1){5.0}}
%   \put(13.5,49.66666666666667){{\tiny det}}
%   \put(88.5,30.0){\oval(79.3855421686747,66.66666666666667)[t]}
%   \put(48.80722891566265,35.0){\vector(0,-1){5.0}}
%   \put(73.5,66.33333333333334){{\tiny nsubj}}
%   \put(111.5,30.0){\oval(28.89189189189189,33.333333333333336)[t]}
%   \put(97.05405405405405,35.0){\vector(0,-1){5.0}}
%   \put(96.5,49.66666666666667){{\tiny cop}}
%   \put(135.0,90.0){\vector(0,-1){60.0}}
%   \put(140.0,80.0){{\tiny root}}
% \end{picture}

%% the cat sleeps
% \setlength{\unitlength}{0.2mm}
% \begin{picture}(167.0,70.0)
%   \put(0.0,0.0){the}
%   \put(37.0,0.0){cat}
%   \put(83.0,0.0){sleeps}
%   \put(0.0,15.0){{\tiny DET}}
%   \put(37.0,15.0){{\tiny NOUN}}
%   \put(83.0,15.0){{\tiny VERB}}
%   \put(28.5,30.0){\oval(28.89189189189189,33.333333333333336)[t]}
%   \put(14.054054054054054,35.0){\vector(0,-1){5.0}}
%   \put(13.5,49.66666666666667){{\tiny det}}
%   \put(70.0,30.0){\oval(39.47826086956522,33.333333333333336)[t]}
%   \put(50.26086956521739,35.0){\vector(0,-1){5.0}}
%   \put(55.0,49.66666666666667){{\tiny nsubj}}
%   \put(98.0,70.0){\vector(0,-1){40.0}}
%   \put(103.0,60.0){{\tiny root}}
% \end{picture}


    \begin{dependency}
        \begin{deptext}[column sep=0.4cm]
              the \& cat \& sleeps \\
            {\tt DET}\&{\tt NOUN}\&{\tt VERB}\\
        \end{deptext}
        \depedge{2}{1}{det}
        \depedge{3}{2}{nsubj}
        \deproot{3}{root}
    \end{dependency} \\

% \end{center}
\caption{The phrase ``the cat sleeps'' analyzed as a UD tree.} % Figure text below figure
\label{fig:cat_sleeps_ud}
\end{figure}

In order to create a tree from a sentence, UD uses supervised machine-learning trained on a large library of manually tagged sentences.\footnote{This differs from the Large Language Models like GPT, which are based on unsupervised learning trained on untagged text from the internet.}


% Not mine: Universal Dependencies (UD) is a framework for representing the syntactic structure of natural language sentences. It is an annotation scheme that provides a universal set of part-of-speech tags and dependency labels that can be applied across different languages.


% TODO: Robust för ogrammatiska meningar osv och ger alltid ett resultat

% \todo[inline]{Talk about the machine learning aspect of UD}

% One way to describe the structure of a sentence is through so-called dependency trees where the words in a sentence form a tree with one word is designated as the root and all the other words attach based on their relation to other words

% Using Dependency trees is a way of doing things \todo{write something here}

% Universal Dependencies is a specific dep tree thing for NLG \todo{and here}

% Universal Dependencies\cite{mcdonald-al-2013} (UD) is another grammar formalism which uses machine-learning for parsing, which allows it to use context more in order to guess which interpretation was intended. The training data is based on a large set of sentences, which have been manually tagged with labels and a tree structure.

%  - abstr trees, GF
\section{Abstract syntax trees and Grammatical Framework}

Another way of describing the grammatical structure of a sentence is through abstract syntax trees, where instead of using words as the nodes in the tree, you use nodes tagged with functions for combining different parts of speech and with the words only being in the leaves. \todo[inline]{make this not be wrong}

% \todo[inline]{Write about abstract trees}
% \todo[inline]{Add an image}

\ac{GF}\cite{ranta-2004} is a formalism for describing natural language grammars as code, which allows converting between natural language and a language-independent abstract syntax. It is split into a generic resource grammar that covers morphology and the language as a whole and application grammars for a more narrow domain which allows a more semantic abstract syntax. When you try to write a grammar that covers a very wide domain, you will often get an over-generating grammar where each sentence can be parsed in many different ways into many different trees, where usually only one is the intended way.

In \autoref{fig:cat_sleeps_gf}, the sentence ``The cat sleeps'' is analyzed as a GF tree. Instead of having dependencies between words like in UD, the trees is built up of constituency. In this case we have a sentence clause (labeled \verb|PredVP : Cl|) that consists of a noun phrase (labeled \verb|DetCN : NP|) and a verb phrase (labeled \verb|UseV : VP|) and the noun phrase in turn consists of a determiner (``the'') and what Grammatical Framework calls a \emph{common noun} (labeled \verb|UseN : CN|) which in turn consists of the noun ``cat''. The verb ``sleeps'' constitutes the verb phrase.

In order to build up this tree, we used these functions:
\begin{itemize}
    \item  \verb|UseN : N -> CN| is a function that takes a noun and converts it to a common noun,
    \item \verb|DetCN : Det -> CN -> NP| is a function that takes a determiner and a common noun and converts it to a noun phrase,
    \item \verb|UseV : V -> VP| is a function that takes a verb and converts it to a verb phrase
    \item and finally, \verb|PredVP : NP -> VP -> Cl| is a function that takes a noun phrase and a verb phrase and converts it to a clause
\end{itemize}

\begin{figure}[htb]
  \centering
  % p -cat=S "the cat sleeps"  | vt
  % \includegraphics[width=0.5\linewidth]{figure/cat_is_black_gf.eps}
  \includegraphics[width=0.5\linewidth]{figure/cat_sleeps_cl.eps}
  \caption{The sentence ``The cat sleeps'' analyzed as a GF tree}
  \label{fig:cat_sleeps_gf}
\end{figure}


% NOTES:
% "The cat is black" vs "The cat is not black" corresponds to PPos vs PNeg
% ud2gf would have trouble differentiating between these because it's only in parameters
% Macros UseCl_NegCop and UseCl_Sim corresponds to the different versions
% These allows the information to be preserved, despite there not being any explicit GF function for the word ``not''
%
% Negative:
% UseCl_NegCop (StrNeg "not") (PredVP (DetCN_theSg (StrThe "the") (UseN cat_N)) (UseAP_Cop (StrCop "be") (PositA black_A)))
% Positive:
% UseCl_Sim (PredVP (DetCN_theSg (StrThe "the") (UseN cat_N)) (UseAP_Cop (StrCop "be") (PositA black_A)))

% #auxfun UseCl_Sim cl : Cl -> S = UseCl TSim PPos cl ; head
% #auxfun UseCl_Ant have cl : Have -> Cl -> S = UseCl TAnt PPos cl ; aux head
% #auxfun UseCl_NegSim do neg cl : Do -> Neg -> Cl -> S = UseCl TSim PNeg cl ; aux advmod head
% #auxfun UseCl_NegAnt have neg cl : Have -> Neg -> Cl -> S = UseCl TAnt PNeg cl ; aux advmod head
% #auxfun UseCl_NegCop neg cl : Neg -> Cl -> S = UseCl TSim PNeg cl ; advmod head

% #auxfun UseAP_Cop cop comp : Cop -> AP -> VP = UseAP comp ; cop head

% #auxcat Cop AUX
% #auxcat Do AUX
% #auxcat Have AUX
% #auxcat Neg PART

% Syncategorematic words:
% #word not not Polarity=Neg
% #word does  do  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin
% #word is   be  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin
% #word would would  VerbForm=Fin
% #word will  will VerbForm=Fin

% Syncategorematic lemmas:
% #lemma UseCl,UseQCl,ImpVP not Neg advmod head
% #lemma UseAP,UseAdv,UseNP be Cop cop head
% #lemma PredVP have Have aux head
% #lemma PredVP,ImpVP do Do aux head

%  - strengths and weaknesses
%
\section{Differences between GF and UD}
% Strengths and weaknesses


% % Grammatical Framework has [some] advantages for [use-case], so it would be useful to be able to parse using UD and then get out a GF tree. A
GF is very strict when it comes to following grammar and spelling, which means that it will often refuse to parse sentences if they contain even the smallest error. UD on the other hand uses machine-learning to give some parse tree for all sentences regardless of how many errors they have.

While the machine-learning-based approach of UD allows it to guess the correct tree better for ambiguous sentences and allows it to handle grammatically incorrect sentences better, GF is much more capable when
it comes to performing transformations on the sentences, while maintaining correct morphology and grammar. This makes it attractive to parse sentences
using UD and then convert the parsed trees to GF trees in order to perform further transformations.


% Anteckningar

% UD har en massa manuellt annoterade träd som folk har tränat maskininlärningssystem på

% GF är mer strikt och kräver att grammatiken följs exakt och stavfel och små grammatikfel kommer inte

% UD är mer robust för fel och kommer alltid ge resultat oavsett om saker inte följs exakt och ger sin bästa gissning

% GF är mer exakt och förutsägbar, så buggar går bättre att anylysera och fixa

% Med GF kan man transformera meningar och träd på mycket fler sätt, till exempel kan ett påstående göras till en fråga bara genom att wrappa hela meningen med en MkQuestion funktion och då kommer automatiskt GF fixa alla morfosyntaktiska ändringar, så som att ändra ordföljd eller

% använder automatiskt rätt genus om man byter ut ett ord till ett annat

% abstrakt träd är oberoende av språk

% RGL beskriver morfosyntax för språk medan en applikationsgrammatik kan vara på högre nivå och ge mer ideomatiska översättningar och det beskrivs i termer av den abstrakta syntaxen för resursgrammatiken



% % Maybe mention the macros in the labels files

% % There exists a naive implementation based on a brute-force algorithm \cite{kolachina-ranta-2017}

% % Briefly describe and motivate the project, and convince the reader of the importance of the proposed thesis work.
% % A good introduction will answer these questions: Why is addressing these challenges significant for gaining new knowledge
% % in the studied domain? How and where can this new knowledge be applied?


\subsection{Useful synthesis between UD and GF}

% \todo[inline]{explain about gf-ud, the old-old version, the new-old version and my developments on that}

Prior to this work there existed a proof-of-concept implementation of a tool for converting between the trees for GF and UD,
with the help of so-called labels-files containing annotations which describe the mapping between UD labels and GF functions,
called gf-ud, which contains both a component for converting from UD to GF, called ud2gf\cite{kolachina-ranta-2017}\footnotemark[1]
and a component for converting from GF to UD, called gf2ud\cite{kolachina-ranta-2016}\footnotemark[1]. This work will focus on the ud2gf component.

\footnotetext[1]{These references apply to an older version of gf-ud, from before the one this thesis is based on. A part of the goal of this thesis is to document the later version in addition to documenting the changes made in the process of this thesis.}

% \todo{these references apply to an older version of gf-ud, from before the one I was working on. a part of the goal of this thesis is to document the old-new version that my changes was based on}

% The labels files can also contain macros, which allows constructing virtual GF functions during the translation from UD, which will be expanded to real GF functions at the end of the translation.
% These can, among other uses, be used to preserve information from the UD labels about subtrees, which can then be used at later points of the transformation to ensure that the desired GF tree is produced.

\subsection{Applications for the synthesis of UD and GF}

The gf-ud tool has been used for both translation and semantics\cite{ranta-al-2020}. %\todo{actually explain this}
Another application has been in concept alignment\cite{masciolini-ranta-2021}.
It has also been used to analyze legal texts as a part of a Controlled Natural Language for law\cite{listenmaa-etal-2021-towards}.

\section{What problem does ud2gf solve?}
% \todo[inline]{This is the main focus of the intro}
%
% \todo[inline]{Add edge labels to graph}
%
% \todo[inline]{write this text}

One problem within \ac{NLP} is converting from text to logic. There are several different paths one could take, each with their own issues, see \autoref{fig:text-to-logic} for an overview. The tool gf2ud makes it possible to get the robust parsing available for dependency trees and the ease of conversion to logic of Abstract Syntax Trees.

\begin{figure}[H]
    \centering
    % \includegraphics{}
    % \missingfigure{Graph of possible ways to convert from text to logic}
    \begin{tikzpicture}
        \tikzset{snake it/.style={decorate, decoration=snake}}
		\node(Text) at (-5, 0) {Text};
		\node(DT) at (-1.5, 0.75) {Dependency Tree};
		\node(AST) at (1.5, -0.75) {Abstract Syntax Tree};
		\node(Logic) at (5, 0) {Logic};
		\draw[->] (Text.north) to node[above] {UDPipe} (DT.west);
		\draw[->, snake it] (Text.east) to (AST.west);
		\draw[->] (AST.east) to (Logic.south);
		\draw[->, snake it] (DT.east) to (Logic.west);
		\draw[->,dashed] (DT.south) to node[right] {ud2gf} (AST.north) ;
    \end{tikzpicture}
    \caption{An overview of how gf2ud can be helpful in converting from text to logic}
    \label{fig:text-to-logic}
\end{figure}

Converting from text to dependency trees is a solved problem\cite{Nivre2006,straka-etal-2016-udpipe,kanerva-etal-2020-turku}. Converting from AST to logic is a solved problem\cite{Montague1973-MONTPT-4,ranta-2004b,ranta2022end}. Converting from dependency trees to logic is a difficult problem\cite{reddy2016transforming,ranta2017explainable}. Converting from natural language text directly to AST is a difficult problem\cite{ranta-2011c,bernardy-stergios-2017,landin-1966-next-700,curry1961some,Montague1973-MONTPT-4}. By converting from DT to AST ud2gf allows us to use the two solved problems to get a path from text to logic.


% Från mallen


% This chapter presents the section levels that can be used in the template.
%
% \section{Section levels}
% \autoref{tab:sections} presents an overview of the section levels that are used in this document. The number of levels that are numbered and included in the table of contents is set in the settings file \texttt{Settings.tex}. The levels are shown in Section \ref{Section_ref}.
%
% This is a new paragraph and should have proper parskip or indentation. Don't forget to cite your sources~\cite{listenmaa-etal-2021-towards}. % '~' becomes space which cannot line break.
%
% \begin{table}[h]
% \centering
% \caption{Section levels} % Table text above table.
% \begin{tabular}{ll} \hline
% Name & Command\\ \hline
% Chapter & \textbackslash\texttt{chapter\{\emph{Chapter name}\}}\\
% Section & \textbackslash\texttt{section\{\emph{Section name}\}}\\
% Subsection & \textbackslash\texttt{subsection\{\emph{Subsection name}\}}\\
% Subsubsection & \textbackslash\texttt{subsubsection\{\emph{Subsubsection name}\}}\\
% %Paragraph & \textbackslash\texttt{paragraph\{\emph{Paragraph name}\}}\\
% %Subparagraph & \textbackslash\texttt{paragraph\{\emph{Subparagraph name}\}}\\ \hline\hline
% \end{tabular}
% \label{tab:sections}
% \end{table}

% \section{Section} \label{Section_ref}
% \subsection{Subsection}
% \subsubsection{Subsubsection}
% \paragraph{Paragraph}
% \subparagraph{Subparagraph}
